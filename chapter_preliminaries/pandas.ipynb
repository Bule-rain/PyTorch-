{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bule-rain/PyTorch-/blob/main/chapter_preliminaries/pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3766a2be",
      "metadata": {
        "origin_pos": 1,
        "id": "3766a2be"
      },
      "source": [
        "# Data Preprocessing\n",
        ":label:`sec_pandas`\n",
        "\n",
        "So far, we have been working with synthetic data\n",
        "that arrived in ready-made tensors.\n",
        "However, to apply deep learning in the wild\n",
        "we must extract messy data\n",
        "stored in arbitrary formats,\n",
        "and preprocess it to suit our needs.\n",
        "Fortunately, the *pandas* [library](https://pandas.pydata.org/)\n",
        "can do much of the heavy lifting.\n",
        "This section, while no substitute\n",
        "for a proper *pandas* [tutorial](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html),\n",
        "will give you a crash course\n",
        "on some of the most common routines.\n",
        "\n",
        "## Reading the Dataset\n",
        "\n",
        "Comma-separated values (CSV) files are ubiquitous\n",
        "for the storing of tabular (spreadsheet-like) data.\n",
        "In them, each line corresponds to one record\n",
        "and consists of several (comma-separated) fields, e.g.,\n",
        "\"Albert Einstein,March 14 1879,Ulm,Federal polytechnic school,field of gravitational physics\".\n",
        "To demonstrate how to load CSV files with `pandas`,\n",
        "we (**create a CSV file below**) `../data/house_tiny.csv`.\n",
        "This file represents a dataset of homes,\n",
        "where each row corresponds to a distinct home\n",
        "and the columns correspond to the number of rooms (`NumRooms`),\n",
        "the roof type (`RoofType`), and the price (`Price`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bea3f1b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:31:20.494944Z",
          "iopub.status.busy": "2023-08-18T19:31:20.494290Z",
          "iopub.status.idle": "2023-08-18T19:31:20.505610Z",
          "shell.execute_reply": "2023-08-18T19:31:20.504677Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "2bea3f1b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
        "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
        "with open(data_file, 'w') as f:\n",
        "    f.write('''NumRooms,RoofType,Price\n",
        "NA,NA,127500\n",
        "2,NA,106000\n",
        "4,Slate,178100\n",
        "NA,NA,140000''')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a73772a6",
      "metadata": {
        "origin_pos": 3,
        "id": "a73772a6"
      },
      "source": [
        "Now let's import `pandas` and load the dataset with `read_csv`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae201e1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:31:20.510380Z",
          "iopub.status.busy": "2023-08-18T19:31:20.509849Z",
          "iopub.status.idle": "2023-08-18T19:31:21.105668Z",
          "shell.execute_reply": "2023-08-18T19:31:21.104596Z"
        },
        "origin_pos": 4,
        "tab": [
          "pytorch"
        ],
        "id": "9ae201e1",
        "outputId": "11e64297-394b-47a8-d38f-847fda833746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   NumRooms RoofType   Price\n",
            "0       NaN      NaN  127500\n",
            "1       2.0      NaN  106000\n",
            "2       4.0    Slate  178100\n",
            "3       NaN      NaN  140000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_file)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bcb4eec",
      "metadata": {
        "origin_pos": 5,
        "id": "2bcb4eec"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "In supervised learning, we train models\n",
        "to predict a designated *target* value,\n",
        "given some set of *input* values.\n",
        "Our first step in processing the dataset\n",
        "is to separate out columns corresponding\n",
        "to input versus target values.\n",
        "We can select columns either by name or\n",
        "via integer-location based indexing (`iloc`).\n",
        "\n",
        "You might have noticed that `pandas` replaced\n",
        "all CSV entries with value `NA`\n",
        "with a special `NaN` (*not a number*) value.\n",
        "This can also happen whenever an entry is empty,\n",
        "e.g., \"3,,,270000\".\n",
        "These are called *missing values*\n",
        "and they are the \"bed bugs\" of data science,\n",
        "a persistent menace that you will confront\n",
        "throughout your career.\n",
        "Depending upon the context,\n",
        "missing values might be handled\n",
        "either via *imputation* or *deletion*.\n",
        "Imputation replaces missing values\n",
        "with estimates of their values\n",
        "while deletion simply discards\n",
        "either those rows or those columns\n",
        "that contain missing values.\n",
        "\n",
        "Here are some common imputation heuristics.\n",
        "[**For categorical input fields,\n",
        "we can treat `NaN` as a category.**]\n",
        "Since the `RoofType` column takes values `Slate` and `NaN`,\n",
        "`pandas` can convert this column\n",
        "into two columns `RoofType_Slate` and `RoofType_nan`.\n",
        "A row whose roof type is `Slate` will set values\n",
        "of `RoofType_Slate` and `RoofType_nan` to 1 and 0, respectively.\n",
        "The converse holds for a row with a missing `RoofType` value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92e80b6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:31:21.109879Z",
          "iopub.status.busy": "2023-08-18T19:31:21.109243Z",
          "iopub.status.idle": "2023-08-18T19:31:21.120081Z",
          "shell.execute_reply": "2023-08-18T19:31:21.119081Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "f92e80b6",
        "outputId": "6a863de1-ee01-4e03-b1b0-9389c3d0364d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   NumRooms  RoofType_Slate  RoofType_nan\n",
            "0       NaN           False          True\n",
            "1       2.0           False          True\n",
            "2       4.0            True         False\n",
            "3       NaN           False          True\n"
          ]
        }
      ],
      "source": [
        "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
        "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea48715",
      "metadata": {
        "origin_pos": 7,
        "id": "fea48715"
      },
      "source": [
        "For missing numerical values,\n",
        "one common heuristic is to\n",
        "[**replace the `NaN` entries with\n",
        "the mean value of the corresponding column**].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37e8e900",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:31:21.123941Z",
          "iopub.status.busy": "2023-08-18T19:31:21.123273Z",
          "iopub.status.idle": "2023-08-18T19:31:21.132513Z",
          "shell.execute_reply": "2023-08-18T19:31:21.131522Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "37e8e900",
        "outputId": "4e05a4af-cc44-457e-f547-7c6b4b581f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   NumRooms  RoofType_Slate  RoofType_nan\n",
            "0       3.0           False          True\n",
            "1       2.0           False          True\n",
            "2       4.0            True         False\n",
            "3       3.0           False          True\n"
          ]
        }
      ],
      "source": [
        "inputs = inputs.fillna(inputs.mean())\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d535cf",
      "metadata": {
        "origin_pos": 9,
        "id": "a6d535cf"
      },
      "source": [
        "## Conversion to the Tensor Format\n",
        "\n",
        "Now that [**all the entries in `inputs` and `targets` are numerical,\n",
        "we can load them into a tensor**] (recall :numref:`sec_ndarray`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d211233b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:31:21.137043Z",
          "iopub.status.busy": "2023-08-18T19:31:21.136126Z",
          "iopub.status.idle": "2023-08-18T19:31:23.159251Z",
          "shell.execute_reply": "2023-08-18T19:31:23.158224Z"
        },
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "d211233b",
        "outputId": "4f8be8eb-07d9-4276-c650-b279713423d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[3., 0., 1.],\n",
              "         [2., 0., 1.],\n",
              "         [4., 1., 0.],\n",
              "         [3., 0., 1.]], dtype=torch.float64),\n",
              " tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor(inputs.to_numpy(dtype=float))\n",
        "y = torch.tensor(targets.to_numpy(dtype=float))\n",
        "X, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O6mGpFPCZuDo"
      },
      "id": "O6mGpFPCZuDo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2a3027b6",
      "metadata": {
        "origin_pos": 14,
        "id": "2a3027b6"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "You now know how to partition data columns,\n",
        "impute missing variables,\n",
        "and load `pandas` data into tensors.\n",
        "In :numref:`sec_kaggle_house`, you will\n",
        "pick up some more data processing skills.\n",
        "While this crash course kept things simple,\n",
        "data processing can get hairy.\n",
        "For example, rather than arriving in a single CSV file,\n",
        "our dataset might be spread across multiple files\n",
        "extracted from a relational database.\n",
        "For instance, in an e-commerce application,\n",
        "customer addresses might live in one table\n",
        "and purchase data in another.\n",
        "Moreover, practitioners face myriad data types\n",
        "beyond categorical and numeric, for example,\n",
        "text strings, images,\n",
        "audio data, and point clouds.\n",
        "Oftentimes, advanced tools and efficient algorithms\n",
        "are required in order to prevent data processing from becoming\n",
        "the biggest bottleneck in the machine learning pipeline.\n",
        "These problems will arise when we get to\n",
        "computer vision and natural language processing.\n",
        "Finally, we must pay attention to data quality.\n",
        "Real-world datasets are often plagued\n",
        "by outliers, faulty measurements from sensors, and recording errors,\n",
        "which must be addressed before\n",
        "feeding the data into any model.\n",
        "Data visualization tools such as [seaborn](https://seaborn.pydata.org/),\n",
        "[Bokeh](https://docs.bokeh.org/), or [matplotlib](https://matplotlib.org/)\n",
        "can help you to manually inspect the data\n",
        "and develop intuitions about\n",
        "the type of problems you may need to address.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Try loading datasets, e.g., Abalone from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php) and inspect their properties. What fraction of them has missing values? What fraction of the variables is numerical, categorical, or text?\n",
        "1. Try indexing and selecting data columns by name rather than by column number. The pandas documentation on [indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) has further details on how to do this.\n",
        "1. How large a dataset do you think you could load this way? What might be the limitations? Hint: consider the time to read the data, representation, processing, and memory footprint. Try this out on your laptop. What happens if you try it out on a server?\n",
        "1. How would you deal with data that has a very large number of categories? What if the category labels are all unique? Should you include the latter?\n",
        "1. What alternatives to pandas can you think of? How about [loading NumPy tensors from a file](https://numpy.org/doc/stable/reference/generated/numpy.load.html)? Check out [Pillow](https://python-pillow.org/), the Python Imaging Library.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\"  # 数据集网址\n",
        "data = pd.read_csv(url, header=None)\n",
        "missing_values = data.isnull().sum().sum()\n",
        "total_values = data.size\n",
        "missing_proportion = missing_values / total_values if total_values > 0 else 0\n",
        "print(f\"缺失值比例: {missing_proportion}\")\n",
        "numeric_columns = data.select_dtypes(include='number').columns\n",
        "categorical_columns = data.select_dtypes(include='object').columns\n",
        "text_columns = []  # 此数据集中无纯文本列\n",
        "\n",
        "numeric_proportion = len(numeric_columns) / len(data.columns) if len(data.columns) > 0 else 0\n",
        "categorical_proportion = len(categorical_columns) / len(data.columns) if len(data.columns) > 0 else 0\n",
        "text_proportion = len(text_columns) / len(data.columns) if len(data.columns) > 0 else 0\n",
        "\n",
        "print(f\"数值型变量比例: {numeric_proportion}\")\n",
        "print(f\"分类型变量比例: {categorical_proportion}\")\n",
        "print(f\"文本型变量比例: {text_proportion}\")"
      ],
      "metadata": {
        "id": "fRZtbJYcZw-t",
        "outputId": "f2dbf1e2-7ae1-43d4-fc4c-4ebcb9cdbed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fRZtbJYcZw-t",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "缺失值比例: 0.0\n",
            "数值型变量比例: 0.8888888888888888\n",
            "分类型变量比例: 0.1111111111111111\n",
            "文本型变量比例: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\n",
        "data.columns = column_names\n",
        "selected_columns = data[['Length', 'Diameter']]\n",
        "print(selected_columns)"
      ],
      "metadata": {
        "id": "nOISOghsav_Y",
        "outputId": "3437cdec-3cdd-4454-c91f-e17f91144b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nOISOghsav_Y",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Length  Diameter\n",
            "0      0.455     0.365\n",
            "1      0.350     0.265\n",
            "2      0.530     0.420\n",
            "3      0.440     0.365\n",
            "4      0.330     0.255\n",
            "...      ...       ...\n",
            "4172   0.565     0.450\n",
            "4173   0.590     0.440\n",
            "4174   0.600     0.475\n",
            "4175   0.625     0.485\n",
            "4176   0.710     0.555\n",
            "\n",
            "[4177 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.你认为以这种方式可以加载多大的数据集？可能存在哪些限制？提示：考虑读取数据的时间、表示方式、处理过程和内存占用。在你的笔记本电脑上试试。如果在服务器上尝试会发生什么？\n",
        "在笔记本电脑上，能加载的数据集大小受到以下限制：\n",
        "内存限制：笔记本电脑的内存通常有限（例如 8GB、16GB 等），如果数据集太大，可能无法全部加载到内存中。当数据集接近或超过可用内存时，会导致内存不足错误（MemoryError）。\n",
        "读取时间：对于非常大的数据集，从存储设备（如硬盘、固态硬盘）读取数据到内存会花费很长时间。机械硬盘的读取速度相对较慢，固态硬盘会快一些，但随着数据量增加，读取时间仍然会显著增加。\n",
        "处理过程：加载数据集后，进行数据处理（如缺失值处理、转换等）也需要时间和内存资源。复杂的处理操作可能会使笔记本电脑的性能大幅下降，甚至导致程序无响应。\n",
        "在服务器上：\n",
        "服务器通常有更多的内存和计算资源，所以理论上可以加载更大的数据集。但也存在限制，例如服务器的内存虽然比笔记本电脑大，但也是有限的，如果数据集过大，同样会面临内存不足的问题。\n",
        "服务器的网络带宽也可能成为限制因素，如果数据集是从远程存储读取，网络传输速度会影响数据加载时间。\n",
        "此外，服务器上可能同时运行多个任务，资源竞争也会影响数据集的加载和处理。"
      ],
      "metadata": {
        "id": "M8Up25wEbL1z"
      },
      "id": "M8Up25wEbL1z"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OVMrssq7bcm_"
      },
      "id": "OVMrssq7bcm_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.你将如何处理具有大量类别的数据？如果类别标签都是唯一的怎么办？是否应该包含后者？\n",
        "处理具有大量类别的数据：\n",
        "降维技术：可以使用主成分分析（PCA）等降维方法，将高维的类别数据转换为低维表示，减少数据维度，同时保留数据的主要特征。\n",
        "聚类方法：将相似的类别聚合成少数几个大的类别，简化数据结构。例如使用 K-Means 聚类算法对类别进行聚类。\n",
        "特征选择：选择对目标变量影响较大的类别特征，去除那些对结果影响较小的类别特征。\n",
        "如果类别标签都是唯一的：\n",
        "这种情况下，直接使用独热编码会导致特征维度急剧增加，可能会引起维度灾难和过拟合问题。\n",
        "是否应该包含后者：\n",
        "一般不建议直接包含所有唯一的类别标签作为特征。可以考虑先对这些标签进行分析，看是否能找到一些潜在的规律或分组方式，将它们进行合并或转换。例如，根据业务逻辑或数据的某些属性，将一些相似的唯一标签归为一类。如果无法进行有效的合并或转换，也可以尝试使用一些专门处理高维稀疏数据的方法，如稀疏矩阵表示等，但这些方法也会增加处理的复杂性。"
      ],
      "metadata": {
        "id": "zwcjilRsbguM"
      },
      "id": "zwcjilRsbguM"
    },
    {
      "cell_type": "markdown",
      "id": "634223ee",
      "metadata": {
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "634223ee"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/29)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.你能想到 pandas 的哪些替代方案？看看 Pillow，即 Python 图像处理库。\n",
        "替代 pandas 的方案：\n",
        "Dask：一个用于并行计算的库，可以处理比内存更大的数据集。它提供了与 pandas 类似的数据结构（如 Dask DataFrame），并且支持并行计算，能够在多个核心或多台机器上处理数据，适合处理大规模数据集。\n",
        "Vaex：一个用于 lazy Out-of-Core DataFrames 的库，它可以在不将整个数据集加载到内存的情况下进行数据处理和分析。Vaex 特别适合处理大型表格数据，提供了高效的计算方法和可视化工具。\n",
        "Polars：一个快速的、多线程的、内存高效的 DataFrame 库，它的性能在处理大型数据集时通常优于 pandas，并且支持多种数据格式的读写。\n",
        "Pillow（Python 图像处理库）：\n",
        "Pillow 主要用于处理图像数据，例如读取、写入、编辑图像等。它提供了丰富的图像操作功能，如调整图像大小、裁剪、旋转、颜色转换等。在处理图像数据时，Pillow 是一个非常有用的工具，可以与深度学习库（如 PyTorch、TensorFlow）结合使用，对图像进行预处理，以便输入到模型中进行训练或预测。例如："
      ],
      "metadata": {
        "id": "I7eCPQYyb1TH"
      },
      "id": "I7eCPQYyb1TH"
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# 打开图像\n",
        "image = Image.open('example.jpg')\n",
        "# 调整图像大小\n",
        "resized_image = image.resize((224, 224))\n",
        "# 转换为灰度图像\n",
        "gray_image = resized_image.convert('L')"
      ],
      "metadata": {
        "id": "stbw5TVSb6wE",
        "outputId": "6cca223d-eb93-452b-a3ca-8cf0d8520602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "id": "stbw5TVSb6wE",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'example.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-dd2025548f13>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 打开图像\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# 调整图像大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresized_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3465\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3466\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3467\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'example.jpg'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}