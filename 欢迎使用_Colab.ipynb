{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bule-rain/PyTorch-/blob/main/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# 深度学习相关库\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import py7zr  # 需要安装: !pip install py7zr\n",
        "from google.colab import drive\n",
        "\n",
        "# 设置随机种子\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# CIFAR-10类别标签\n",
        "CLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def mount_drive_and_check():\n",
        "    \"\"\"挂载Drive并检查文件\"\"\"\n",
        "    print(\"正在挂载Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # 检查cifar-10.zip文件是否存在\n",
        "    zip_path = '/content/drive/MyDrive/cifar-10.zip'\n",
        "    if os.path.exists(zip_path):\n",
        "        print(f\"✅ 找到cifar-10.zip文件: {zip_path}\")\n",
        "        print(f\"文件大小: {os.path.getsize(zip_path) / (1024*1024):.1f} MB\")\n",
        "        return zip_path\n",
        "    else:\n",
        "        print(\"❌ 未找到cifar-10.zip文件\")\n",
        "        print(\"请确保文件路径正确\")\n",
        "        return None\n",
        "\n",
        "def explore_zip_contents(zip_path):\n",
        "    \"\"\"探索zip文件内容\"\"\"\n",
        "    print(\"正在探索zip文件内容...\")\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        print(f\"zip文件中包含 {len(file_list)} 个文件:\")\n",
        "        for file in file_list:\n",
        "            print(f\"  - {file}\")\n",
        "\n",
        "        # 查找7z文件\n",
        "        sevenZ_files = [f for f in file_list if f.endswith('.7z')]\n",
        "        csv_files = [f for f in file_list if f.endswith('.csv')]\n",
        "\n",
        "        print(f\"\\n找到 {len(sevenZ_files)} 个7z文件:\")\n",
        "        for sz_file in sevenZ_files:\n",
        "            print(f\"  - {sz_file}\")\n",
        "\n",
        "        print(f\"找到 {len(csv_files)} 个CSV文件:\")\n",
        "        for csv_file in csv_files:\n",
        "            print(f\"  - {csv_file}\")\n",
        "\n",
        "        return file_list, sevenZ_files, csv_files\n",
        "\n",
        "def extract_and_load_data(zip_path):\n",
        "    \"\"\"解压zip和7z文件，加载 标签CSV + 图片数据\"\"\"\n",
        "    print(\"正在解压文件...\")\n",
        "\n",
        "    # 创建临时目录\n",
        "    temp_dir = '/content/temp_cifar'\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # 首先解压zip文件\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_dir)\n",
        "        print(\"✅ zip文件解压完成\")\n",
        "\n",
        "    # 查找解压后的文件\n",
        "    extracted_files = os.listdir(temp_dir)\n",
        "    print(f\"解压后的文件: {extracted_files}\")\n",
        "\n",
        "    # 初始化数据变量\n",
        "    train_labels_df = None  # 存训练标签\n",
        "    test_images = None      # 存测试图片\n",
        "    sample_submission = None# 存提交样本\n",
        "    train_images = None     # 存训练图片（可选，若需要从7z加载训练图）\n",
        "\n",
        "    # 处理CSV文件（标签、提交样本）\n",
        "    for file in extracted_files:\n",
        "        if file.endswith('.csv'):\n",
        "            file_path = os.path.join(temp_dir, file)\n",
        "            if 'trainLabels' in file:  # 匹配训练标签文件\n",
        "                train_labels_df = pd.read_csv(file_path)\n",
        "                print(f\"✅ 加载训练标签数据: {train_labels_df.shape}\")\n",
        "            elif 'sampleSubmission' in file:  # 匹配提交样本\n",
        "                sample_submission = pd.read_csv(file_path)\n",
        "                print(f\"✅ 加载提交样本: {sample_submission.shape}\")\n",
        "\n",
        "    # 处理7z文件（提取图片）\n",
        "    for file in extracted_files:\n",
        "        if file.endswith('.7z'):\n",
        "            print(f\"正在解压7z文件: {file}\")\n",
        "            sevenZ_path = os.path.join(temp_dir, file)\n",
        "\n",
        "            try:\n",
        "                with py7zr.SevenZipFile(sevenZ_path, mode='r') as archive:\n",
        "                    archive.extractall(path=temp_dir)\n",
        "                    print(f\"✅ {file} 解压完成\")\n",
        "\n",
        "                    # 区分训练/测试7z，加载图片\n",
        "                    if 'train' in file:\n",
        "                        train_img_dir = os.path.join(temp_dir, 'train')  # 假设解压到 train 目录\n",
        "                        if os.path.exists(train_img_dir):\n",
        "                            train_images = []\n",
        "                            for img_name in sorted(os.listdir(train_img_dir)):\n",
        "                                img_path = os.path.join(train_img_dir, img_name)\n",
        "                                img = Image.open(img_path).convert('RGB')\n",
        "                                img = np.array(img)\n",
        "                                train_images.append(img)\n",
        "                            train_images = np.array(train_images)\n",
        "                            print(f\"✅ 加载训练图片 {train_images.shape[0]} 张\")\n",
        "                    elif 'test' in file:\n",
        "                        test_img_dir = os.path.join(temp_dir, 'test')  # 假设解压到 test 目录\n",
        "                        if os.path.exists(test_img_dir):\n",
        "                            test_images = []\n",
        "                            for img_name in sorted(os.listdir(test_img_dir)):\n",
        "                                img_path = os.path.join(test_img_dir, img_name)\n",
        "                                img = Image.open(img_path).convert('RGB')\n",
        "                                img = np.array(img)\n",
        "                                test_images.append(img)\n",
        "                            test_images = np.array(test_images)\n",
        "                            print(f\"✅ 加载测试图片 {test_images.shape[0]} 张\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 解压 {file} 失败: {e}\")\n",
        "                continue\n",
        "\n",
        "    # 整合训练数据（标签 + 图片）\n",
        "    if train_labels_df is not None and train_images is not None:\n",
        "        # 确保标签和图片数量匹配（CIFAR-10 训练集 50000 张图 + 50000 条标签）\n",
        "        if len(train_labels_df) == len(train_images):\n",
        "            train_df = pd.DataFrame({\n",
        "                'id': train_labels_df['id'],\n",
        "                'label': train_labels_df['label'],\n",
        "                'image': list(train_images)  # 存图片数组\n",
        "            })\n",
        "        else:\n",
        "            print(\"⚠️ 训练标签和图片数量不匹配，跳过整合\")\n",
        "            train_df = None\n",
        "    else:\n",
        "        train_df = None\n",
        "\n",
        "    # 构造测试数据DataFrame（仅图片，提交时用ID匹配）\n",
        "    if test_images is not None:\n",
        "        test_df = pd.DataFrame({\n",
        "            'id': range(1, len(test_images)+1),  # 假设ID从1开始\n",
        "            'image': list(test_images)\n",
        "        })\n",
        "    else:\n",
        "        test_df = None\n",
        "\n",
        "    return train_df, test_df, sample_submission, temp_dir\n",
        "\n",
        "def cleanup_temp_files(temp_dir):\n",
        "    \"\"\"清理临时文件\"\"\"\n",
        "    import shutil\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "        print(f\"✅ 临时文件已清理: {temp_dir}\")\n",
        "\n",
        "def load_cifar_data():\n",
        "    \"\"\"完整的数据加载流程（适配图片+标签）\"\"\"\n",
        "    # 安装py7zr（如果没有安装）\n",
        "    try:\n",
        "        import py7zr\n",
        "    except ImportError:\n",
        "        print(\"正在安装py7zr...\")\n",
        "        os.system(\"pip install py7zr\")\n",
        "        import py7zr\n",
        "\n",
        "    # 挂载Drive并检查文件\n",
        "    zip_path = mount_drive_and_check()\n",
        "    if not zip_path:\n",
        "        return None, None, None\n",
        "\n",
        "    # 探索文件内容\n",
        "    file_list, sevenZ_files, csv_files = explore_zip_contents(zip_path)\n",
        "\n",
        "    # 解压并加载数据（图片+标签）\n",
        "    train_df, test_df, sample_submission, temp_dir = extract_and_load_data(zip_path)\n",
        "\n",
        "    # 数据概览\n",
        "    if train_df is not None:\n",
        "        print(f\"\\n📊 训练数据概览:\")\n",
        "        print(f\"形状: {train_df.shape}\")\n",
        "        print(f\"列名: {list(train_df.columns)}\")\n",
        "        print(train_df.head())\n",
        "\n",
        "    if test_df is not None:\n",
        "        print(f\"\\n📊 测试数据概览:\")\n",
        "        print(f\"形状: {test_df.shape}\")\n",
        "        print(f\"列名: {list(test_df.columns)}\")\n",
        "        print(test_df.head())\n",
        "\n",
        "    if sample_submission is not None:\n",
        "        print(f\"\\n📊 提交样本概览:\")\n",
        "        print(f\"形状: {sample_submission.shape}\")\n",
        "        print(f\"列名: {list(sample_submission.columns)}\")\n",
        "        print(sample_submission.head())\n",
        "\n",
        "    # 询问是否清理临时文件\n",
        "    print(f\"\\n临时文件保存在: {temp_dir}\")\n",
        "    print(\"如需清理临时文件，请调用: cleanup_temp_files(temp_dir)\")\n",
        "\n",
        "    return train_df, test_df, sample_submission\n",
        "\n",
        "def quick_data_analysis(train_df, test_df, sample_submission):\n",
        "    \"\"\"快速数据分析（适配图片数据）\"\"\"\n",
        "    print(\"\\n=== 数据分析 ===\")\n",
        "\n",
        "    if train_df is not None:\n",
        "        print(\"训练数据信息:\")\n",
        "        print(f\"  形状: {train_df.shape}\")\n",
        "        print(f\"  列名: {list(train_df.columns)}\")\n",
        "        print(f\"  前几行:\")\n",
        "        print(train_df.head(2))\n",
        "\n",
        "        # 检查标签分布\n",
        "        if 'label' in train_df.columns:\n",
        "            label_counts = train_df['label'].value_counts().sort_index()\n",
        "            print(f\"\\n标签分布:\")\n",
        "            for label, count in label_counts.items():\n",
        "                print(f\"  {label}: {count} 张\")\n",
        "\n",
        "    if test_df is not None:\n",
        "        print(\"\\n测试数据信息:\")\n",
        "        print(f\"  形状: {test_df.shape}\")\n",
        "        print(f\"  列名: {list(test_df.columns)}\")\n",
        "        print(f\"  前几行:\")\n",
        "        print(test_df.head(2))\n",
        "\n",
        "    if sample_submission is not None:\n",
        "        print(\"\\n提交样本信息:\")\n",
        "        print(f\"  形状: {sample_submission.shape}\")\n",
        "        print(f\"  列名: {list(sample_submission.columns)}\")\n",
        "        print(f\"  前几行:\")\n",
        "        print(sample_submission.head(2))\n",
        "\n",
        "def preprocess_data(train_df, test_df):\n",
        "    \"\"\"数据预处理（适配图片数组）\"\"\"\n",
        "    print(\"\\n=== 数据预处理 ===\")\n",
        "\n",
        "    # 创建标签映射字典\n",
        "    label_to_int = {\n",
        "        'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4,\n",
        "        'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9\n",
        "    }\n",
        "\n",
        "    # 分离特征（图片）和标签\n",
        "    if train_df is not None and 'image' in train_df.columns and 'label' in train_df.columns:\n",
        "        X_train = np.array(train_df['image'].tolist())  # 转成 numpy 数组\n",
        "        # 将字符串标签转换为数字标签\n",
        "        y_train = train_df['label'].map(label_to_int).values\n",
        "        print(f\"标签转换完成：{train_df['label'].iloc[0]} -> {y_train[0]}\")\n",
        "    else:\n",
        "        X_train, y_train = None, None\n",
        "\n",
        "    # 处理测试数据（图片）\n",
        "    if test_df is not None and 'image' in test_df.columns:\n",
        "        X_test = np.array(test_df['image'].tolist())\n",
        "        test_ids = test_df['id'].values\n",
        "    else:\n",
        "        X_test, test_ids = None, None\n",
        "\n",
        "    # 重塑为标准图像格式 (32x32x3)\n",
        "    if X_train is not None and X_train.shape[1:] == (32, 32, 3):\n",
        "        print(\"✅ 训练数据已为32x32x3图像格式\")\n",
        "    else:\n",
        "        if X_train is not None:\n",
        "            print(f\"⚠️  训练图像维度异常，实际形状: {X_train.shape}\")\n",
        "\n",
        "    if X_test is not None and X_test.shape[1:] == (32, 32, 3):\n",
        "        print(\"✅ 测试数据已为32x32x3图像格式\")\n",
        "    else:\n",
        "        if X_test is not None:\n",
        "            print(f\"⚠️  测试图像维度异常，实际形状: {X_test.shape}\")\n",
        "\n",
        "    # 标准化到 [0,1] 范围\n",
        "    if X_train is not None:\n",
        "        X_train = X_train.astype('float32') / 255.0\n",
        "        print(f\"✅ 训练数据标准化完成，像素值范围: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
        "    if X_test is not None:\n",
        "        X_test = X_test.astype('float32') / 255.0\n",
        "        print(f\"✅ 测试数据标准化完成，像素值范围: [{X_test.min():.3f}, {X_test.max():.3f}]\")\n",
        "\n",
        "    # 标签one-hot编码\n",
        "    y_train_onehot = keras.utils.to_categorical(y_train, 10) if y_train is not None else None\n",
        "\n",
        "    print(f\"\\n预处理完成:\")\n",
        "    print(f\"  训练图像: {X_train.shape if X_train is not None else 'None'}\")\n",
        "    print(f\"  训练标签: {y_train_onehot.shape if y_train_onehot is not None else 'None'}\")\n",
        "    print(f\"  测试图像: {X_test.shape if X_test is not None else 'None'}\")\n",
        "    print(f\"  标签范围: {y_train.min()}-{y_train.max()}\" if y_train is not None else \"  标签范围: None\")\n",
        "\n",
        "    return X_train, y_train_onehot, X_test, test_ids\n",
        "\n",
        "def create_model():\n",
        "    \"\"\"创建改进的CNN模型\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        # 第一层卷积块\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # 第二层卷积块\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # 第三层卷积块\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # 全连接层\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model_enhanced(model, X_train, y_train):\n",
        "    \"\"\"增强版模型训练函数\"\"\"\n",
        "    print(\"\\n=== 开始训练 ===\")\n",
        "\n",
        "    # 检查数据\n",
        "    if X_train is None or y_train is None:\n",
        "        print(\"❌ 训练数据未正确加载，无法训练\")\n",
        "        return None, None\n",
        "\n",
        "    # 检查数据形状\n",
        "    if len(X_train.shape) != 4 or X_train.shape[1:] != (32, 32, 3):\n",
        "        print(f\"❌ 训练数据形状不正确: {X_train.shape}, 期望: (N, 32, 32, 3)\")\n",
        "        return None, None\n",
        "\n",
        "    if len(y_train.shape) != 2 or y_train.shape[1] != 10:\n",
        "        print(f\"❌ 标签数据形状不正确: {y_train.shape}, 期望: (N, 10)\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"✅ 数据检查通过:\")\n",
        "    print(f\"   训练数据形状: {X_train.shape}\")\n",
        "    print(f\"   标签数据形状: {y_train.shape}\")\n",
        "\n",
        "    # 编译模型\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # 分割验证集\n",
        "        X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "            X_train, y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=np.argmax(y_train, axis=1)\n",
        "        )\n",
        "\n",
        "        print(f\"✅ 数据分割完成:\")\n",
        "        print(f\"   训练集: {X_train_split.shape[0]} 张\")\n",
        "        print(f\"   验证集: {X_val.shape[0]} 张\")\n",
        "\n",
        "        # 数据增强\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=15,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            zoom_range=0.1,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        # 回调函数\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=15,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=7,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        print(\"🚀 开始训练...\")\n",
        "\n",
        "        # 计算步数\n",
        "        batch_size = 32  # 减小批次大小避免内存问题\n",
        "        steps_per_epoch = max(1, len(X_train_split) // batch_size)\n",
        "\n",
        "        # 训练模型\n",
        "        history = model.fit(\n",
        "            datagen.flow(X_train_split, y_train_split, batch_size=batch_size),\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=50,  # 减少初始epochs\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # 训练完成后的评估\n",
        "        print(\"\\n=== 训练完成 ===\")\n",
        "        val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "        print(f\"✅ 最终验证准确率: {val_accuracy:.4f}\")\n",
        "        print(f\"✅ 最终验证损失: {val_loss:.4f}\")\n",
        "\n",
        "        return model, history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 训练过程中出现错误: {str(e)}\")\n",
        "        print(\"可能的解决方案:\")\n",
        "        print(\"1. 检查数据预处理是否正确\")\n",
        "        print(\"2. 减小批次大小\")\n",
        "        print(\"3. 检查GPU内存是否足够\")\n",
        "        return None, None\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"绘制训练历史图表\"\"\"\n",
        "    if history is None:\n",
        "        print(\"❌ 没有训练历史数据可以绘制\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # 准确率图表\n",
        "        ax1.plot(history.history['accuracy'], label='训练准确率')\n",
        "        ax1.plot(history.history['val_accuracy'], label='验证准确率')\n",
        "        ax1.set_title('模型准确率')\n",
        "        ax1.set_xlabel('轮次')\n",
        "        ax1.set_ylabel('准确率')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # 损失图表\n",
        "        ax2.plot(history.history['loss'], label='训练损失')\n",
        "        ax2.plot(history.history['val_loss'], label='验证损失')\n",
        "        ax2.set_title('模型损失')\n",
        "        ax2.set_xlabel('轮次')\n",
        "        ax2.set_ylabel('损失')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 绘图时出现错误: {str(e)}\")\n",
        "\n",
        "def prepare_cifar10_data():\n",
        "    \"\"\"准备CIFAR-10数据（备用方案：直接从Keras加载）\"\"\"\n",
        "    try:\n",
        "        # 加载CIFAR-10数据\n",
        "        (X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "        # 数据预处理\n",
        "        X_train = X_train.astype('float32') / 255.0\n",
        "        X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "        # 标签one-hot编码\n",
        "        y_train_onehot = keras.utils.to_categorical(y_train, 10)\n",
        "        y_test_onehot = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "        print(f\"✅ 数据加载完成:\")\n",
        "        print(f\"   训练集: {X_train.shape}\")\n",
        "        print(f\"   测试集: {X_test.shape}\")\n",
        "        print(f\"   训练标签: {y_train_onehot.shape}\")\n",
        "        print(f\"   测试标签: {y_test_onehot.shape}\")\n",
        "\n",
        "        return X_train, y_train_onehot, X_test, y_test_onehot\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 数据加载失败: {str(e)}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def generate_submission_enhanced(model, X_test, test_ids, sample_submission):\n",
        "    \"\"\"增强版提交文件生成函数\"\"\"\n",
        "    print(\"\\n=== 生成提交文件 ===\")\n",
        "\n",
        "    if X_test is None or sample_submission is None:\n",
        "        print(\"❌ 测试数据或提交样本未正确加载，无法生成提交文件\")\n",
        "        return None\n",
        "\n",
        "    if model is None:\n",
        "        print(\"❌ 模型未训练，无法生成预测\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📊 开始预测 {len(X_test)} 张测试图片...\")\n",
        "\n",
        "    # 分批预测（避免内存不足）\n",
        "    batch_size = 1000\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        end_idx = min(i + batch_size, len(X_test))\n",
        "        batch_predictions = model.predict(X_test[i:end_idx], verbose=0)\n",
        "        predictions.append(batch_predictions)\n",
        "\n",
        "        # 显示进度\n",
        "        progress = (end_idx / len(X_test)) * 100\n",
        "        print(f\"预测进度: {progress:.1f}% ({end_idx}/{len(X_test)})\")\n",
        "\n",
        "    # 合并所有预测结果\n",
        "    predictions = np.vstack(predictions)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # 将数字标签转回字符串标签\n",
        "    int_to_label = {\n",
        "        0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer',\n",
        "        5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'\n",
        "    }\n",
        "\n",
        "    predicted_labels = [int_to_label[pred] for pred in predicted_classes]\n",
        "\n",
        "    # 创建提交文件\n",
        "    submission = sample_submission.copy()\n",
        "    submission['label'] = predicted_labels\n",
        "\n",
        "    # 验证提交文件格式\n",
        "    print(f\"\\n📋 提交文件验证:\")\n",
        "    print(f\"   形状: {submission.shape}\")\n",
        "    print(f\"   列名: {list(submission.columns)}\")\n",
        "    print(f\"   ID范围: {submission['id'].min()} - {submission['id'].max()}\")\n",
        "    print(f\"   标签类型: {submission['label'].dtype}\")\n",
        "\n",
        "    # 检查预测分布\n",
        "    print(f\"\\n📊 预测结果分布:\")\n",
        "    label_counts = submission['label'].value_counts().sort_index()\n",
        "    for label, count in label_counts.items():\n",
        "        percentage = (count / len(submission)) * 100\n",
        "        print(f\"   {label}: {count:,} 张 ({percentage:.1f}%)\")\n",
        "\n",
        "    # 保存到Drive\n",
        "    submission_path = '/content/drive/MyDrive/cifar10_submission.csv'\n",
        "    submission.to_csv(submission_path, index=False)\n",
        "\n",
        "    print(f\"\\n✅ 提交文件已保存: {submission_path}\")\n",
        "    print(f\"📁 文件大小: {os.path.getsize(submission_path) / (1024*1024):.1f} MB\")\n",
        "\n",
        "    # 显示提交文件前几行\n",
        "    print(f\"\\n📝 提交文件预览:\")\n",
        "    print(submission.head(10))\n",
        "\n",
        "    return submission\n",
        "\n",
        "def validate_submission(submission, sample_submission):\n",
        "    \"\"\"验证提交文件的正确性\"\"\"\n",
        "    print(\"\\n=== 提交文件验证 ===\")\n",
        "\n",
        "    # 检查形状\n",
        "    if submission.shape == sample_submission.shape:\n",
        "        print(\"✅ 文件形状正确\")\n",
        "    else:\n",
        "        print(f\"❌ 文件形状错误: {submission.shape} vs {sample_submission.shape}\")\n",
        "        return False\n",
        "\n",
        "    # 检查列名\n",
        "    if list(submission.columns) == list(sample_submission.columns):\n",
        "        print(\"✅ 列名正确\")\n",
        "    else:\n",
        "        print(f\"❌ 列名错误: {list(submission.columns)} vs {list(sample_submission.columns)}\")\n",
        "        return False\n",
        "\n",
        "    # 检查ID是否完整\n",
        "    if set(submission['id']) == set(sample_submission['id']):\n",
        "        print(\"✅ ID完整\")\n",
        "    else:\n",
        "        print(\"❌ ID不完整\")\n",
        "        return False\n",
        "\n",
        "    # 检查标签是否合法\n",
        "    valid_labels = {'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}\n",
        "    if set(submission['label']) <= valid_labels:\n",
        "        print(\"✅ 标签合法\")\n",
        "    else:\n",
        "        invalid_labels = set(submission['label']) - valid_labels\n",
        "        print(f\"❌ 发现非法标签: {invalid_labels}\")\n",
        "        return False\n",
        "\n",
        "    print(\"🎉 提交文件验证通过！\")\n",
        "    return True\n",
        "\n",
        "def main():\n",
        "    \"\"\"主函数 - 完整流程（适配CIFAR-10真实数据）\"\"\"\n",
        "    print(\"🚀 CIFAR-10 图像分类项目开始\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 初始化变量，避免NameError\n",
        "    X_train, y_train_onehot, X_test, test_ids = None, None, None, None\n",
        "    model, history, submission = None, None, None\n",
        "\n",
        "    try:\n",
        "        # 1. 数据加载\n",
        "        print(\"步骤1: 数据加载\")\n",
        "        train_df, test_df, sample_submission = load_cifar_data()\n",
        "\n",
        "        if train_df is None and test_df is None:\n",
        "            print(\"⚠️ 自定义数据加载失败，使用Keras内置CIFAR-10数据集\")\n",
        "            X_train, y_train_onehot, X_test, y_test_onehot = prepare_cifar10_data()\n",
        "            test_ids = list(range(1, len(X_test) + 1))\n",
        "            # 创建简单的提交样本格式\n",
        "            sample_submission = pd.DataFrame({\n",
        "                'id': test_ids,\n",
        "                'label': ['airplane'] * len(test_ids)  # 占位符\n",
        "            })\n",
        "        else:\n",
        "            # 2. 数据分析\n",
        "            print(\"\\n步骤2: 数据分析\")\n",
        "            quick_data_analysis(train_df, test_df, sample_submission)\n",
        "\n",
        "            # 3. 数据预处理\n",
        "            print(\"\\n步骤3: 数据预处理\")\n",
        "            X_train, y_train_onehot, X_test, test_ids = preprocess_data(train_df, test_df)\n",
        "\n",
        "        # 4. 模型创建\n",
        "        print(\"\\n步骤4: 模型创建\")\n",
        "        model = create_model()\n",
        "        print(\"✅ 模型创建完成\")\n",
        "        print(f\"模型参数数量: {model.count_params():,}\")\n",
        "\n",
        "        # 显示模型结构\n",
        "        model.summary()\n",
        "\n",
        "        # 5. 模型训练\n",
        "        print(\"\\n步骤5: 模型训练\")\n",
        "        if X_train is not None and y_train_onehot is not None:\n",
        "            model, history = train_model_enhanced(model, X_train, y_train_onehot)\n",
        "\n",
        "            if history is not None:\n",
        "                # 6. 绘制训练曲线\n",
        "                print(\"\\n步骤6: 训练结果可视化\")\n",
        "                plot_training_history(history)\n",
        "            else:\n",
        "                print(\"❌ 训练失败，无法继续\")\n",
        "                return\n",
        "        else:\n",
        "            print(\"❌ 训练数据不可用，无法训练模型\")\n",
        "            return\n",
        "\n",
        "        # 7. 生成预测和提交文件\n",
        "        print(\"\\n步骤7: 生成提交文件\")\n",
        "        if X_test is not None and sample_submission is not None:\n",
        "            submission = generate_submission_enhanced(model, X_test, test_ids, sample_submission)\n",
        "\n",
        "            if submission is not None:\n",
        "                # 8. 验证提交文件\n",
        "                print(\"\\n步骤8: 验证提交文件\")\n",
        "                is_valid = validate_submission(submission, sample_submission)\n",
        "\n",
        "                if is_valid:\n",
        "                    print(\"🎉 项目完成！提交文件已生成并验证通过\")\n",
        "                else:\n",
        "                    print(\"⚠️ 提交文件验证失败，请检查\")\n",
        "            else:\n",
        "                print(\"❌ 提交文件生成失败\")\n",
        "        else:\n",
        "            print(\"❌ 测试数据不可用，无法生成提交文件\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 执行过程中出现错误: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    finally:\n",
        "        # 清理临时文件（如果存在）\n",
        "        temp_dir = '/content/temp_cifar'\n",
        "        if 'temp_dir' in locals():\n",
        "            try:\n",
        "                cleanup_temp_files(temp_dir)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"程序执行完毕\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 设置matplotlib后端（适配Colab环境）\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')  # 或者 'inline' 如果在Jupyter中\n",
        "\n",
        "    # 执行主函数\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BNOe3ADlbNV",
        "outputId": "41016282-a89d-44d6-91d4-4a9457572941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 CIFAR-10 图像分类项目开始\n",
            "==================================================\n",
            "步骤1: 数据加载\n",
            "正在挂载Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ 找到cifar-10.zip文件: /content/drive/MyDrive/cifar-10.zip\n",
            "文件大小: 715.4 MB\n",
            "正在探索zip文件内容...\n",
            "zip文件中包含 4 个文件:\n",
            "  - sampleSubmission.csv\n",
            "  - test.7z\n",
            "  - train.7z\n",
            "  - trainLabels.csv\n",
            "\n",
            "找到 2 个7z文件:\n",
            "  - test.7z\n",
            "  - train.7z\n",
            "找到 2 个CSV文件:\n",
            "  - sampleSubmission.csv\n",
            "  - trainLabels.csv\n",
            "正在解压文件...\n",
            "✅ zip文件解压完成\n",
            "解压后的文件: ['train.7z', 'test.7z', 'trainLabels.csv', 'test', 'sampleSubmission.csv', 'train']\n",
            "✅ 加载训练标签数据: (50000, 2)\n",
            "✅ 加载提交样本: (300000, 2)\n",
            "正在解压7z文件: train.7z\n",
            "✅ train.7z 解压完成\n",
            "✅ 加载训练图片 50000 张\n",
            "正在解压7z文件: test.7z\n",
            "✅ test.7z 解压完成\n",
            "✅ 加载测试图片 300000 张\n",
            "\n",
            "📊 训练数据概览:\n",
            "形状: (50000, 3)\n",
            "列名: ['id', 'label', 'image']\n",
            "   id       label                                              image\n",
            "0   1        frog  [[[59, 62, 63], [43, 46, 45], [50, 48, 43], [6...\n",
            "1   2       truck  [[[125, 125, 116], [110, 101, 91], [102, 90, 8...\n",
            "2   3       truck  [[[62, 64, 44], [50, 50, 26], [46, 44, 19], [4...\n",
            "3   4        deer  [[[203, 206, 208], [201, 202, 202], [208, 209,...\n",
            "4   5  automobile  [[[62, 55, 7], [61, 55, 7], [60, 55, 6], [59, ...\n",
            "\n",
            "📊 测试数据概览:\n",
            "形状: (300000, 2)\n",
            "列名: ['id', 'image']\n",
            "   id                                              image\n",
            "0   1  [[[134, 116, 78], [144, 127, 91], [145, 128, 9...\n",
            "1   2  [[[130, 119, 139], [131, 119, 140], [132, 120,...\n",
            "2   3  [[[128, 138, 79], [109, 122, 67], [104, 123, 6...\n",
            "3   4  [[[254, 254, 254], [250, 250, 250], [251, 251,...\n",
            "4   5  [[[170, 149, 128], [185, 164, 141], [193, 171,...\n",
            "\n",
            "📊 提交样本概览:\n",
            "形状: (300000, 2)\n",
            "列名: ['id', 'label']\n",
            "   id label\n",
            "0   1   cat\n",
            "1   2   cat\n",
            "2   3   cat\n",
            "3   4   cat\n",
            "4   5   cat\n",
            "\n",
            "临时文件保存在: /content/temp_cifar\n",
            "如需清理临时文件，请调用: cleanup_temp_files(temp_dir)\n",
            "\n",
            "步骤2: 数据分析\n",
            "\n",
            "=== 数据分析 ===\n",
            "训练数据信息:\n",
            "  形状: (50000, 3)\n",
            "  列名: ['id', 'label', 'image']\n",
            "  前几行:\n",
            "   id  label                                              image\n",
            "0   1   frog  [[[59, 62, 63], [43, 46, 45], [50, 48, 43], [6...\n",
            "1   2  truck  [[[125, 125, 116], [110, 101, 91], [102, 90, 8...\n",
            "\n",
            "标签分布:\n",
            "  airplane: 5000 张\n",
            "  automobile: 5000 张\n",
            "  bird: 5000 张\n",
            "  cat: 5000 张\n",
            "  deer: 5000 张\n",
            "  dog: 5000 张\n",
            "  frog: 5000 张\n",
            "  horse: 5000 张\n",
            "  ship: 5000 张\n",
            "  truck: 5000 张\n",
            "\n",
            "测试数据信息:\n",
            "  形状: (300000, 2)\n",
            "  列名: ['id', 'image']\n",
            "  前几行:\n",
            "   id                                              image\n",
            "0   1  [[[134, 116, 78], [144, 127, 91], [145, 128, 9...\n",
            "1   2  [[[130, 119, 139], [131, 119, 140], [132, 120,...\n",
            "\n",
            "提交样本信息:\n",
            "  形状: (300000, 2)\n",
            "  列名: ['id', 'label']\n",
            "  前几行:\n",
            "   id label\n",
            "0   1   cat\n",
            "1   2   cat\n",
            "\n",
            "步骤3: 数据预处理\n",
            "\n",
            "=== 数据预处理 ===\n",
            "标签转换完成：frog -> 6\n",
            "✅ 训练数据已为32x32x3图像格式\n",
            "✅ 测试数据已为32x32x3图像格式\n",
            "✅ 训练数据标准化完成，像素值范围: [0.000, 1.000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 修复后的CIFAR-10模型训练代码\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 6. 创建模型\n",
        "def create_model():\n",
        "    \"\"\"创建改进的CNN模型\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        # 第一层卷积块\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # 第二层卷积块\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # 第三层卷积块\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # 全连接层\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# 7. 训练模型\n",
        "def train_model_enhanced(model, X_train, y_train):\n",
        "    \"\"\"增强版模型训练函数\"\"\"\n",
        "    print(\"\\n=== 开始训练 ===\")\n",
        "\n",
        "    # 检查数据\n",
        "    if X_train is None or y_train is None:\n",
        "        print(\"❌ 训练数据未正确加载，无法训练\")\n",
        "        return None, None\n",
        "\n",
        "    # 检查数据形状\n",
        "    if len(X_train.shape) != 4 or X_train.shape[1:] != (32, 32, 3):\n",
        "        print(f\"❌ 训练数据形状不正确: {X_train.shape}, 期望: (N, 32, 32, 3)\")\n",
        "        return None, None\n",
        "\n",
        "    if len(y_train.shape) != 2 or y_train.shape[1] != 10:\n",
        "        print(f\"❌ 标签数据形状不正确: {y_train.shape}, 期望: (N, 10)\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"✅ 数据检查通过:\")\n",
        "    print(f\"   训练数据形状: {X_train.shape}\")\n",
        "    print(f\"   标签数据形状: {y_train.shape}\")\n",
        "\n",
        "    # 编译模型\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # 分割验证集\n",
        "        X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "            X_train, y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=np.argmax(y_train, axis=1)\n",
        "        )\n",
        "\n",
        "        print(f\"✅ 数据分割完成:\")\n",
        "        print(f\"   训练集: {X_train_split.shape[0]} 张\")\n",
        "        print(f\"   验证集: {X_val.shape[0]} 张\")\n",
        "\n",
        "        # 数据增强\n",
        "        datagen = ImageDataGenerator(\n",
        "            rotation_range=15,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            zoom_range=0.1,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        # 回调函数\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=15,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=7,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        print(\"🚀 开始训练...\")\n",
        "\n",
        "        # 计算步数\n",
        "        batch_size = 32  # 减小批次大小避免内存问题\n",
        "        steps_per_epoch = max(1, len(X_train_split) // batch_size)\n",
        "\n",
        "        # 训练模型\n",
        "        history = model.fit(\n",
        "            datagen.flow(X_train_split, y_train_split, batch_size=batch_size),\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=50,  # 减少初始epochs\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # 训练完成后的评估\n",
        "        print(\"\\n=== 训练完成 ===\")\n",
        "        val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "        print(f\"✅ 最终验证准确率: {val_accuracy:.4f}\")\n",
        "        print(f\"✅ 最终验证损失: {val_loss:.4f}\")\n",
        "\n",
        "        return model, history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 训练过程中出现错误: {str(e)}\")\n",
        "        print(\"可能的解决方案:\")\n",
        "        print(\"1. 检查数据预处理是否正确\")\n",
        "        print(\"2. 减小批次大小\")\n",
        "        print(\"3. 检查GPU内存是否足够\")\n",
        "        return None, None\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"绘制训练历史图表\"\"\"\n",
        "    if history is None:\n",
        "        print(\"❌ 没有训练历史数据可以绘制\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # 准确率图表\n",
        "        ax1.plot(history.history['accuracy'], label='训练准确率')\n",
        "        ax1.plot(history.history['val_accuracy'], label='验证准确率')\n",
        "        ax1.set_title('模型准确率')\n",
        "        ax1.set_xlabel('轮次')\n",
        "        ax1.set_ylabel('准确率')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # 损失图表\n",
        "        ax2.plot(history.history['loss'], label='训练损失')\n",
        "        ax2.plot(history.history['val_loss'], label='验证损失')\n",
        "        ax2.set_title('模型损失')\n",
        "        ax2.set_xlabel('轮次')\n",
        "        ax2.set_ylabel('损失')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 绘图时出现错误: {str(e)}\")\n",
        "\n",
        "# 数据准备函数（如果需要）\n",
        "def prepare_cifar10_data():\n",
        "    \"\"\"准备CIFAR-10数据\"\"\"\n",
        "    try:\n",
        "        # 加载CIFAR-10数据\n",
        "        (X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "        # 数据预处理\n",
        "        X_train = X_train.astype('float32') / 255.0\n",
        "        X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "        # 标签one-hot编码\n",
        "        y_train_onehot = keras.utils.to_categorical(y_train, 10)\n",
        "        y_test_onehot = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "        print(f\"✅ 数据加载完成:\")\n",
        "        print(f\"   训练集: {X_train.shape}\")\n",
        "        print(f\"   测试集: {X_test.shape}\")\n",
        "        print(f\"   训练标签: {y_train_onehot.shape}\")\n",
        "        print(f\"   测试标签: {y_test_onehot.shape}\")\n",
        "\n",
        "        return X_train, y_train_onehot, X_test, y_test_onehot\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 数据加载失败: {str(e)}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# 主执行代码\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 50)\n",
        "    print(\"CIFAR-10 CNN模型训练\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 步骤1: 准备数据（如果需要）\n",
        "    print(\"\\n步骤1：准备数据\")\n",
        "    print(\"-\" * 30)\n",
        "    # X_train, y_train_onehot, X_test, y_test_onehot = prepare_cifar10_data()\n",
        "\n",
        "    # 步骤2: 创建模型\n",
        "    print(\"\\n步骤2：创建模型\")\n",
        "    print(\"-\" * 30)\n",
        "    model = create_model()\n",
        "    print(f\"✅ 模型创建完成\")\n",
        "    print(f\"📊 模型参数量: {model.count_params():,}\")\n",
        "    print(f\"📋 模型结构:\")\n",
        "    model.summary()\n",
        "\n",
        "    # 步骤3: 训练模型（需要确保X_train和y_train_onehot已定义）\n",
        "    print(\"\\n步骤3：训练模型\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # 检查变量是否存在\n",
        "    try:\n",
        "        # 这里假设X_train和y_train_onehot已经在之前的代码中定义\n",
        "        # 如果没有，请取消注释上面的数据准备代码\n",
        "        model_trained, history = train_model_enhanced(model, X_train, y_train_onehot)\n",
        "\n",
        "        if model_trained is not None and history is not None:\n",
        "            print(\"✅ 模型训练成功完成！\")\n",
        "\n",
        "            # 绘制训练历史\n",
        "            plot_training_history(history)\n",
        "\n",
        "            # 保存模型\n",
        "            try:\n",
        "                model_path = 'cifar10_model.h5'\n",
        "                model_trained.save(model_path)\n",
        "                print(f\"✅ 模型已保存到: {model_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 模型保存失败: {str(e)}\")\n",
        "                print(\"尝试保存为SavedModel格式...\")\n",
        "                try:\n",
        "                    model_trained.save('cifar10_model')\n",
        "                    print(\"✅ 模型已保存为SavedModel格式\")\n",
        "                except Exception as e2:\n",
        "                    print(f\"❌ SavedModel保存也失败: {str(e2)}\")\n",
        "        else:\n",
        "            print(\"❌ 模型训练失败\")\n",
        "\n",
        "    except NameError as e:\n",
        "        print(f\"❌ 变量未定义: {str(e)}\")\n",
        "        print(\"请确保X_train和y_train_onehot已经正确加载和预处理\")\n",
        "        print(\"可以取消注释数据准备代码来加载CIFAR-10数据\")"
      ],
      "metadata": {
        "id": "A0iTuB67trim",
        "outputId": "c5392f07-fd27-4e32-f738-4f32b8467bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "CIFAR-10 CNN模型训练\n",
            "==================================================\n",
            "\n",
            "步骤1：准备数据\n",
            "------------------------------\n",
            "\n",
            "步骤2：创建模型\n",
            "------------------------------\n",
            "✅ 模型创建完成\n",
            "📊 模型参数量: 490,922\n",
            "📋 模型结构:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m66,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m490,922\u001b[0m (1.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">490,922</span> (1.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m488,938\u001b[0m (1.87 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">488,938</span> (1.87 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,984\u001b[0m (7.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> (7.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "步骤3：训练模型\n",
            "------------------------------\n",
            "❌ 变量未定义: name 'X_train' is not defined\n",
            "请确保X_train和y_train_onehot已经正确加载和预处理\n",
            "可以取消注释数据准备代码来加载CIFAR-10数据\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 训练模型\n",
        "def train_model_enhanced(model, X_train, y_train):\n",
        "    \"\"\"增强版模型训练函数\"\"\"\n",
        "    print(\"\\n=== 开始训练 ===\")\n",
        "\n",
        "    if X_train is None or y_train is None:\n",
        "        print(\"❌ 训练数据未正确加载，无法训练\")\n",
        "        return None, None\n",
        "\n",
        "    # 编译模型\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 分割验证集\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "        X_train, y_train, test_size=0.2, random_state=42, stratify=np.argmax(y_train, axis=1)\n",
        "    )\n",
        "\n",
        "    print(f\"✅ 数据分割完成:\")\n",
        "    print(f\"   训练集: {X_train_split.shape[0]} 张\")\n",
        "    print(f\"   验证集: {X_val.shape[0]} 张\")\n",
        "\n",
        "    # 数据增强\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # 回调函数\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=7,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    print(\"🚀 开始训练...\")\n",
        "\n",
        "    # 训练模型\n",
        "    history = model.fit(\n",
        "        datagen.flow(X_train_split, y_train_split, batch_size=64),\n",
        "        steps_per_epoch=len(X_train_split) // 64,\n",
        "        epochs=100,  # 设置较大值，依靠EarlyStopping自动停止\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 训练完成后的评估\n",
        "    print(\"\\n=== 训练完成 ===\")\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
        "    print(f\"✅ 最终验证准确率: {val_accuracy:.4f}\")\n",
        "    print(f\"✅ 最终验证损失: {val_loss:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"绘制训练历史图表\"\"\"\n",
        "    if history is None:\n",
        "        return\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    # 准确率图表\n",
        "    ax1.plot(history.history['accuracy'], label='训练准确率')\n",
        "    ax1.plot(history.history['val_accuracy'], label='验证准确率')\n",
        "    ax1.set_title('模型准确率')\n",
        "    ax1.set_xlabel('轮次')\n",
        "    ax1.set_ylabel('准确率')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # 损失图表\n",
        "    ax2.plot(history.history['loss'], label='训练损失')\n",
        "    ax2.plot(history.history['val_loss'], label='验证损失')\n",
        "    ax2.set_title('模型损失')\n",
        "    ax2.set_xlabel('轮次')\n",
        "    ax2.set_ylabel('损失')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 执行步骤7\n",
        "print(\"=\" * 50)\n",
        "print(\"步骤7：训练模型\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 训练模型\n",
        "model, history = train_model_enhanced(model, X_train, y_train_onehot)\n",
        "\n",
        "if model is not None and history is not None:\n",
        "    print(\"✅ 模型训练成功完成！\")\n",
        "\n",
        "    # 绘制训练历史\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # 保存模型\n",
        "    model_path = '/content/drive/MyDrive/cifar10_model.h5'\n",
        "    model.save(model_path)\n",
        "    print(f\"✅ 模型已保存到: {model_path}\")\n",
        "else:\n",
        "    print(\"❌ 模型训练失败\")"
      ],
      "metadata": {
        "id": "WvHgx3XnuNDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. 生成提交文件\n",
        "def generate_submission_enhanced(model, X_test, test_ids, sample_submission):\n",
        "    \"\"\"增强版提交文件生成函数\"\"\"\n",
        "    print(\"\\n=== 生成提交文件 ===\")\n",
        "\n",
        "    if X_test is None or sample_submission is None:\n",
        "        print(\"❌ 测试数据或提交样本未正确加载，无法生成提交文件\")\n",
        "        return None\n",
        "\n",
        "    if model is None:\n",
        "        print(\"❌ 模型未训练，无法生成预测\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📊 开始预测 {len(X_test)} 张测试图片...\")\n",
        "\n",
        "    # 分批预测（避免内存不足）\n",
        "    batch_size = 1000\n",
        "    predictions = []\n",
        "\n",
        "    for i in range(0, len(X_test), batch_size):\n",
        "        end_idx = min(i + batch_size, len(X_test))\n",
        "        batch_predictions = model.predict(X_test[i:end_idx], verbose=0)\n",
        "        predictions.append(batch_predictions)\n",
        "\n",
        "        # 显示进度\n",
        "        progress = (end_idx / len(X_test)) * 100\n",
        "        print(f\"预测进度: {progress:.1f}% ({end_idx}/{len(X_test)})\")\n",
        "\n",
        "    # 合并所有预测结果\n",
        "    predictions = np.vstack(predictions)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # 将数字标签转回字符串标签\n",
        "    int_to_label = {\n",
        "        0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer',\n",
        "        5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'\n",
        "    }\n",
        "\n",
        "    predicted_labels = [int_to_label[pred] for pred in predicted_classes]\n",
        "\n",
        "    # 创建提交文件\n",
        "    submission = sample_submission.copy()\n",
        "    submission['label'] = predicted_labels\n",
        "\n",
        "    # 验证提交文件格式\n",
        "    print(f\"\\n📋 提交文件验证:\")\n",
        "    print(f\"   形状: {submission.shape}\")\n",
        "    print(f\"   列名: {list(submission.columns)}\")\n",
        "    print(f\"   ID范围: {submission['id'].min()} - {submission['id'].max()}\")\n",
        "    print(f\"   标签类型: {submission['label'].dtype}\")\n",
        "\n",
        "    # 检查预测分布\n",
        "    print(f\"\\n📊 预测结果分布:\")\n",
        "    label_counts = submission['label'].value_counts().sort_index()\n",
        "    for label, count in label_counts.items():\n",
        "        percentage = (count / len(submission)) * 100\n",
        "        print(f\"   {label}: {count:,} 张 ({percentage:.1f}%)\")\n",
        "\n",
        "    # 保存到Drive\n",
        "    submission_path = '/content/drive/MyDrive/cifar10_submission.csv'\n",
        "    submission.to_csv(submission_path, index=False)\n",
        "\n",
        "    print(f\"\\n✅ 提交文件已保存: {submission_path}\")\n",
        "    print(f\"📁 文件大小: {os.path.getsize(submission_path) / (1024*1024):.1f} MB\")\n",
        "\n",
        "    # 显示提交文件前几行\n",
        "    print(f\"\\n📝 提交文件预览:\")\n",
        "    print(submission.head(10))\n",
        "\n",
        "    return submission\n",
        "\n",
        "def validate_submission(submission, sample_submission):\n",
        "    \"\"\"验证提交文件的正确性\"\"\"\n",
        "    print(\"\\n=== 提交文件验证 ===\")\n",
        "\n",
        "    # 检查形状\n",
        "    if submission.shape == sample_submission.shape:\n",
        "        print(\"✅ 文件形状正确\")\n",
        "    else:\n",
        "        print(f\"❌ 文件形状错误: {submission.shape} vs {sample_submission.shape}\")\n",
        "        return False\n",
        "\n",
        "    # 检查列名\n",
        "    if list(submission.columns) == list(sample_submission.columns):\n",
        "        print(\"✅ 列名正确\")\n",
        "    else:\n",
        "        print(f\"❌ 列名错误: {list(submission.columns)} vs {list(sample_submission.columns)}\")\n",
        "        return False\n",
        "\n",
        "    # 检查ID是否完整\n",
        "    if set(submission['id']) == set(sample_submission['id']):\n",
        "        print(\"✅ ID完整\")\n",
        "    else:\n",
        "        print(\"❌ ID不完整\")\n",
        "        return False\n",
        "\n",
        "    # 检查标签是否合法\n",
        "    valid_labels = {'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'}\n",
        "    if set(submission['label']) <= valid_labels:\n",
        "        print(\"✅ 标签合法\")\n",
        "    else:\n",
        "        invalid_labels = set(submission['label']) - valid_labels\n",
        "        print(f\"❌ 发现非法标签: {invalid_labels}\")\n",
        "        return False\n",
        "\n",
        "    print(\"🎉 提交文件验证通过！\")\n",
        "    return True\n",
        "\n",
        "# 执行步骤8\n",
        "print(\"=\" * 50)\n",
        "print(\"步骤8：生成提交文件\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 生成提交文件\n",
        "submission = generate_submission_enhanced(model, X_test, test_ids, sample_submission)\n",
        "\n",
        "if submission is not None:\n",
        "    # 验证提交文件\n",
        "    is_valid = validate_submission(submission, sample_submission)\n",
        "\n",
        "    if is_valid:\n",
        "        print(\"\\n🎉 任务完成！\")\n",
        "        print(\"=\" * 50)\n",
        "        print(\"✅ 模型训练完成\")\n",
        "        print(\"✅ 提交文件生成成功\")\n",
        "        print(\"✅ 文件验证通过\")\n",
        "        print(\"🚀 可以直接提交到Kaggle!\")\n",
        "\n",
        "        # 最终统计\n",
        "        print(f\"\\n📈 最终统计:\")\n",
        "        print(f\"   训练样本: 50,000 张\")\n",
        "        print(f\"   测试样本: 300,000 张\")\n",
        "        print(f\"   模型参数: {model.count_params():,}\")\n",
        "        print(f\"   提交文件: /content/drive/MyDrive/cifar10_submission.csv\")\n",
        "    else:\n",
        "        print(\"❌ 提交文件验证失败\")\n",
        "else:\n",
        "    print(\"❌ 提交文件生成失败\")"
      ],
      "metadata": {
        "id": "m9kH-D1JuQE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import py7zr  # 需先安装：!pip install py7zr\n",
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive（若文件在 Drive 中，需此步骤访问文件）\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. 解压 cifar-10.zip 到当前目录（已有的解压逻辑，可根据实际情况调整）\n",
        "zip_path = '/content/drive/MyDrive/cifar-10.zip'\n",
        "extract_dir = '.'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(\"✅ zip 文件已解压到当前目录\")\n",
        "\n",
        "# 2. 定义 7z 文件路径（假设解压后 test.7z 和 train.7z 在当前目录）\n",
        "test_7z_path = os.path.join(extract_dir, 'test.7z')\n",
        "train_7z_path = os.path.join(extract_dir, 'train.7z')\n",
        "\n",
        "# 3. 解压 test.7z\n",
        "try:\n",
        "    with py7zr.SevenZipFile(test_7z_path, mode='r') as archive:\n",
        "        archive.extractall(path=extract_dir)\n",
        "    print(\"✅ test.7z 已成功解压\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 解压 test.7z 失败: {e}\")\n",
        "\n",
        "# 4. 解压 train.7z\n",
        "try:\n",
        "    with py7zr.SevenZipFile(train_7z_path, mode='r') as archive:\n",
        "        archive.extractall(path=extract_dir)\n",
        "    print(\"✅ train.7z 已成功解压\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 解压 train.7z 失败: {e}\")"
      ],
      "metadata": {
        "id": "mPAmIBVqSsrw",
        "outputId": "abd59951-a334-4708-8061-ba71b415955f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ zip 文件已解压到当前目录\n",
            "✅ test.7z 已成功解压\n",
            "✅ train.7z 已成功解压\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 步骤1：安装py7zr\n",
        "!pip install py7zr"
      ],
      "metadata": {
        "id": "1wcrQnRHNyqt",
        "outputId": "ff67bae0-2043-4b2c-b01a-5ba668c6d7c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py7zr\n",
            "  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting texttable (from py7zr)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pycryptodomex>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from py7zr) (3.23.0)\n",
            "Collecting brotli>=1.1.0 (from py7zr)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from py7zr) (5.9.5)\n",
            "Collecting pyzstd>=0.16.1 (from py7zr)\n",
            "  Downloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting pyppmd<1.3.0,>=1.1.0 (from py7zr)\n",
            "  Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n",
            "  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n",
            "  Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.13.2 in /usr/local/lib/python3.11/dist-packages (from pyzstd>=0.16.1->py7zr) (4.14.0)\n",
            "Downloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pybcj, multivolumefile, inflate64, py7zr\n",
            "Successfully installed brotli-1.1.0 inflate64-1.0.3 multivolumefile-0.2.3 py7zr-1.0.0 pybcj-1.0.6 pyppmd-1.2.0 pyzstd-0.17.0 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMkaBMEunbpD",
        "outputId": "da8ac05a-45f6-4bf5-8c0b-28273830b00f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "欢迎使用 Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}